{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45308831-37af-4e39-9dd7-6759b38f4dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING FINAL DATA ASSEMBLY ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pyreadstat\n",
    "import gc\n",
    "\n",
    "print(\"--- STARTING FINAL DATA ASSEMBLY ---\")\n",
    "\n",
    "# --- STEP 1: SETUP ---\n",
    "data_folder = 'HCES23-24'\n",
    "merge_keys = ['FSU_Serial_No', 'Panel', 'Sub_sample', 'Sample_Household_No']\n",
    "\n",
    "household_feature_files = [\n",
    "    'LEVEL - 01(Section 1 and 1.1).sav',\n",
    "    'LEVEL - 03.sav',\n",
    "    'LEVEL - 04 (Section 4.1).sav',\n",
    "    'LEVEL - 07 (Section 4.2).sav',\n",
    "    'LEVEL - 11 (Section 4.3).sav'\n",
    "]\n",
    "person_level_file = 'LEVEL - 02 (Section 3).sav'\n",
    "level15_file = 'LEVEL - 15 (Section 1.1, A2,B2 & C2).sav'\n",
    "\n",
    "expenditure_files = [\n",
    "    'LEVEL - 05 ( Sec 5 & 6).sav', \n",
    "    'LEVEL - 06 (Section 7).sav',\n",
    "    'LEVEL - 08 (Section 8.1).sav',\n",
    "    'LEVEL - 09 (Section 9 & 10 & 11).sav',\n",
    "    'LEVEL - 10 (Section 12).sav', \n",
    "    'LEVEL - 12 (Section 13).sav'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43419b43-05ca-4410-bc45-e4e813358331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1 COMPLETE: SETUP ---\n",
      "--- Analysis of MONTHLY_CONSUMPTION_EXP from Level 15 ---\n",
      "count    785859.000000\n",
      "mean      12894.407002\n",
      "std        7653.676532\n",
      "min         200.000000\n",
      "25%        8000.000000\n",
      "50%       11500.000000\n",
      "75%       16000.000000\n",
      "max      155000.000000\n",
      "Name: MONTHLY_CONSUMPTION_EXP, dtype: float64\n",
      "\n",
      "Top 10 most common values for MONTHLY_CONSUMPTION_EXP:\n",
      "MONTHLY_CONSUMPTION_EXP\n",
      "NaN        261953\n",
      "10000.0     48419\n",
      "12000.0     45905\n",
      "15000.0     44313\n",
      "8000.0      39636\n",
      "9000.0      30202\n",
      "14000.0     26693\n",
      "20000.0     25283\n",
      "18000.0     24241\n",
      "7000.0      24128\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- STEP 2 COMPLETE: CORE HOUSEHOLD DATA ASSEMBLED ---\n",
      "\n",
      "--- STEP 3 COMPLETE: PERSON-LEVEL FEATURES ADDED ---\n",
      "Engineered Asset_Score_X1 using 9 columns.\n",
      "Engineered Scheme_Index_X2 using 6 columns.\n",
      "\n",
      "--- STEP 4 COMPLETE: EXPENDITURE FEATURES ADDED ---\n",
      "\n",
      "Shape before dropping missing target values: (227114, 141)\n",
      "Shape after dropping missing target values: (227114, 141)\n",
      "\n",
      "--- STEP 5 COMPLETE: TARGET VARIABLE CREATED & CLEANED FILE SAVED ---\n",
      "\n",
      "--- DATA ASSEMBLY IS NOW 100% COMPLETE! ---\n",
      "\n",
      "Final assembled data sample:\n",
      "   Scheme_Index_X2  Asset_Score_X1  household_size_calculated  \\\n",
      "0              4.0            14.0                          9   \n",
      "1              2.0             6.0                          5   \n",
      "2              4.0            15.0                          5   \n",
      "3              3.0             5.0                          3   \n",
      "4              2.0             6.0                          5   \n",
      "\n",
      "   head_of_household_age  fuel_expenditure  comm_expenditure         MPCE  \n",
      "0                     54            2495.0            1770.0  2444.444444  \n",
      "1                     37             758.0             480.0  4200.000000  \n",
      "2                     34            1802.0             500.0  5000.000000  \n",
      "3                     85             942.0             450.0  5333.333333  \n",
      "4                     32             797.0             450.0  3800.000000  \n",
      "\n",
      "--- Descriptive Statistics of Final Assembled Data ---\n",
      "       Scheme_Index_X2  Asset_Score_X1  household_size_calculated  \\\n",
      "count    227114.000000   227114.000000              227114.000000   \n",
      "mean          2.152307        5.559979                   4.875177   \n",
      "std           0.921827        4.242918                   3.088290   \n",
      "min           1.000000        0.000000                   1.000000   \n",
      "25%           2.000000        2.000000                   3.000000   \n",
      "50%           2.000000        5.000000                   4.000000   \n",
      "75%           2.000000        8.000000                   6.000000   \n",
      "max           5.000000       24.000000                  45.000000   \n",
      "\n",
      "       fuel_expenditure  comm_expenditure  MONTHLY_CONSUMPTION_EXP  \\\n",
      "count     227114.000000     227114.000000            227114.000000   \n",
      "mean        1466.494049        592.175383             14003.866191   \n",
      "std         1092.488552        507.317403              8011.041612   \n",
      "min            0.000000          0.000000               200.000000   \n",
      "25%          841.000000        255.000000              8700.000000   \n",
      "50%         1213.000000        478.000000             12400.000000   \n",
      "75%         1720.000000        720.000000             17000.000000   \n",
      "max        24512.000000      14309.000000            155000.000000   \n",
      "\n",
      "                MPCE  \n",
      "count  227114.000000  \n",
      "mean     3588.161685  \n",
      "std      2580.789439  \n",
      "min       200.000000  \n",
      "25%      2000.000000  \n",
      "50%      3000.000000  \n",
      "75%      4333.333333  \n",
      "max     65000.000000  \n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if pd.api.types.is_numeric_dtype(col_type):\n",
    "            c_min, c_max = df[col].min(), df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max: df[col] = df[col].astype(np.float32)\n",
    "                else: df[col] = df[col].astype(np.float64)\n",
    "        elif col_type == 'object':\n",
    "            if df[col].nunique() / len(df) < 0.5: df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "print(\"\\n--- STEP 1 COMPLETE: SETUP ---\")\n",
    "\n",
    "\n",
    "# --- STEP 2: LOAD AND MERGE CORE HOUSEHOLD DATA ---\n",
    "base_df = pd.read_spss(os.path.join(data_folder, household_feature_files[0]))\n",
    "base_df = reduce_mem_usage(base_df)\n",
    "base_df.drop_duplicates(subset=merge_keys, inplace=True)\n",
    "for key in merge_keys:\n",
    "    if key in base_df.columns: base_df[key] = pd.to_numeric(base_df[key], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "\n",
    "for file_name in household_feature_files[1:]:\n",
    "    df_to_merge = pd.read_spss(os.path.join(data_folder, file_name))\n",
    "    df_to_merge = reduce_mem_usage(df_to_merge)\n",
    "    df_to_merge.drop_duplicates(subset=merge_keys, inplace=True)\n",
    "    for key in merge_keys:\n",
    "        if key in df_to_merge.columns: df_to_merge[key] = pd.to_numeric(df_to_merge[key], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    \n",
    "    new_cols = df_to_merge.columns.difference(base_df.columns)\n",
    "    base_df = pd.merge(base_df, df_to_merge[merge_keys + new_cols.tolist()], on=merge_keys, how='left')\n",
    "    del df_to_merge; gc.collect()\n",
    "\n",
    "# --- NEW LOGIC: PROCESS LEVEL-15 SEPARATELY AND INTELLIGENTLY ---\n",
    "df_l15 = pd.read_spss(os.path.join(data_folder, level15_file))\n",
    "# Add this cell in your notebook right after loading 'LEVEL - 15.sav'\n",
    "print(\"--- Analysis of MONTHLY_CONSUMPTION_EXP from Level 15 ---\")\n",
    "\n",
    "# Show basic statistics\n",
    "print(df_l15['MONTHLY_CONSUMPTION_EXP'].describe())\n",
    "\n",
    "# Show the most common values (this will tell us if it's mostly zero)\n",
    "print(\"\\nTop 10 most common values for MONTHLY_CONSUMPTION_EXP:\")\n",
    "print(df_l15['MONTHLY_CONSUMPTION_EXP'].value_counts(dropna=False).head(10))\n",
    "for key in merge_keys:\n",
    "    if key in df_l15.columns: df_l15[key] = pd.to_numeric(df_l15[key], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "\n",
    "# Group by household and take the MAX value (this will grab the real number and ignore NaNs)\n",
    "level15_summary = df_l15.groupby(merge_keys, observed=True)[['MONTHLY_CONSUMPTION_EXP', 'HOUSEHOLD_SIZE']].max().reset_index()\n",
    "base_df = pd.merge(base_df, level15_summary, on=merge_keys, how='left')\n",
    "del df_l15, level15_summary; gc.collect()\n",
    "\n",
    "print(\"\\n--- STEP 2 COMPLETE: CORE HOUSEHOLD DATA ASSEMBLED ---\")\n",
    "\n",
    "\n",
    "# --- STEP 3: PROCESS AND MERGE PERSON-LEVEL DATA ---\n",
    "person_df = pd.read_spss(os.path.join(data_folder, person_level_file))\n",
    "person_df = reduce_mem_usage(person_df)\n",
    "person_df['Age'] = pd.to_numeric(person_df['Age'], errors='coerce')\n",
    "person_df['Years_of_Education'] = pd.to_numeric(person_df['Years_of_Education'], errors='coerce')\n",
    "person_df['Used_Internet_Last_30_Days'] = pd.to_numeric(person_df['Used_Internet_Last_30_Days'], errors='coerce').replace(2, 0)\n",
    "for key in merge_keys:\n",
    "    if key in person_df.columns: person_df[key] = pd.to_numeric(person_df[key], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "\n",
    "household_size = person_df.groupby(merge_keys, observed=True).size().rename('household_size_calculated')\n",
    "HEAD_OF_HOUSEHOLD_LABEL = 'self' \n",
    "head_age = person_df[person_df['Relation_to_Head'] == HEAD_OF_HOUSEHOLD_LABEL].groupby(merge_keys, observed=True)['Age'].first().rename('head_of_household_age')\n",
    "adults_df = person_df[person_df['Age'] >= 18]\n",
    "avg_edu_adults = adults_df.groupby(merge_keys, observed=True)['Years_of_Education'].mean().rename('avg_education_years_adults')\n",
    "num_internet_users = person_df.groupby(merge_keys, observed=True)['Used_Internet_Last_30_Days'].sum().rename('num_internet_users')\n",
    "\n",
    "person_agg_feats = pd.concat([household_size, head_age, avg_edu_adults, num_internet_users], axis=1).reset_index()\n",
    "base_df = pd.merge(base_df, person_agg_feats, on=merge_keys, how='left')\n",
    "del person_df, adults_df, person_agg_feats; gc.collect()\n",
    "\n",
    "print(\"\\n--- STEP 3 COMPLETE: PERSON-LEVEL FEATURES ADDED ---\")\n",
    "\n",
    "\n",
    "# --- STEP 4: EXTRACT SPECIFIC EXPENDITURE FEATURES (UNCHANGED) ---\n",
    "# ... (The full, two-phase CSV-based code for Step 4 remains the same as the last version I sent) ...\n",
    "# For brevity, I am not re-pasting the entire Step 4 block here, please use the last full version I provided.\n",
    "# If you need it again, let me know.\n",
    "FUEL_ITEM_CODES = [332, 338, 331, 334, 335, 341, 343, 337, 333, 344, 345, 340, 336, 342]\n",
    "COMM_ITEM_CODES = [488, 487, 496, 490]\n",
    "def find_column_names(df):\n",
    "    item_col, value_col = None, None\n",
    "    for col in df.columns:\n",
    "        if 'item_code' in col.lower(): item_col = col\n",
    "        if 'value' in col.lower(): value_col = col\n",
    "    return item_col, value_col\n",
    "temp_dir = 'cleaned_chunks_csv'\n",
    "if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
    "os.makedirs(temp_dir)\n",
    "chunk_counter = 0\n",
    "for file_name in expenditure_files:\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    chunk_size, offset = 1_000_000, 0\n",
    "    while True:\n",
    "        try:\n",
    "            df_chunk, meta = pyreadstat.read_sav(file_path, row_offset=offset, row_limit=chunk_size)\n",
    "            if df_chunk.empty: break\n",
    "            item_col_name, value_col_name = find_column_names(df_chunk)\n",
    "            if not item_col_name or not value_col_name:\n",
    "                offset += chunk_size\n",
    "                continue\n",
    "            keys_in_chunk = [key for key in merge_keys if key in df_chunk.columns]\n",
    "            df_clean_chunk = df_chunk[keys_in_chunk + [item_col_name, value_col_name]].copy()\n",
    "            df_clean_chunk.rename(columns={item_col_name: 'item_code', value_col_name: 'value'}, inplace=True)\n",
    "            df_clean_chunk.to_csv(os.path.join(temp_dir, f'chunk_{chunk_counter}.csv'), index=False)\n",
    "            chunk_counter += 1; offset += chunk_size; del df_chunk, df_clean_chunk; gc.collect()\n",
    "        except Exception as e:\n",
    "            break\n",
    "all_fuel_chunks, all_comm_chunks = [], []\n",
    "cleaned_chunk_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.csv')]\n",
    "for file_path in cleaned_chunk_files:\n",
    "    df_clean_chunk = pd.read_csv(file_path, dtype={key: str for key in merge_keys if key in pd.read_csv(file_path, nrows=0).columns})\n",
    "    all_fuel_chunks.append(df_clean_chunk[df_clean_chunk['item_code'].isin(FUEL_ITEM_CODES)].groupby(merge_keys, observed=True)['value'].sum())\n",
    "    all_comm_chunks.append(df_clean_chunk[df_clean_chunk['item_code'].isin(COMM_ITEM_CODES)].groupby(merge_keys, observed=True)['value'].sum())\n",
    "    del df_clean_chunk; gc.collect()\n",
    "total_fuel_spending = pd.concat(all_fuel_chunks).groupby(level=list(range(len(merge_keys)))).sum()\n",
    "total_comm_spending = pd.concat(all_comm_chunks).groupby(level=list(range(len(merge_keys)))).sum()\n",
    "shutil.rmtree(temp_dir)\n",
    "fuel_df_final = total_fuel_spending.rename('fuel_expenditure').reset_index()\n",
    "comm_df_final = total_comm_spending.rename('comm_expenditure').reset_index()\n",
    "for key in merge_keys:\n",
    "    if key in fuel_df_final.columns: fuel_df_final[key] = pd.to_numeric(fuel_df_final[key], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    if key in comm_df_final.columns: comm_df_final[key] = pd.to_numeric(comm_df_final[key], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "base_df = pd.merge(base_df, fuel_df_final, on=merge_keys, how='left')\n",
    "base_df = pd.merge(base_df, comm_df_final, on=merge_keys, how='left')\n",
    "base_df['fuel_expenditure'] = base_df['fuel_expenditure'].fillna(0)\n",
    "base_df['comm_expenditure'] = base_df['comm_expenditure'].fillna(0)\n",
    "# =========================================================================\n",
    "# === NEW LOGIC: FEATURE ENGINEERING FOR COMPOSITE SCORES (X1 and X2) ===\n",
    "# =========================================================================\n",
    "\n",
    "# 1. ASSET SCORE (X1): Weighted Sum of Possessions\n",
    "\n",
    "# Define Asset Columns and Weights (Adjust these weights based on your domain knowledge!)\n",
    "asset_weights = {\n",
    "    'Possess_Car': 5,\n",
    "    'Possess_Truck': 5,\n",
    "    'Possess_WashingMachine': 3,\n",
    "    'Possess_Laptop': 3,\n",
    "    'Possess_Refrigerator': 2,\n",
    "    'Possess_Television': 2,\n",
    "    'Possess_AirCooler': 1,\n",
    "    'Possess_Bicycle': 1,\n",
    "    'Possess_Scooter': 2\n",
    "}\n",
    "\n",
    "# Find all actual 'Possess_' columns present in the dataframe\n",
    "possess_cols = [col for col in base_df.columns if col.startswith('Possess_')]\n",
    "asset_cols_to_use = base_df.columns.intersection(asset_weights.keys()).tolist()\n",
    "\n",
    "# Ensure binary columns are numeric (1/0) and fill NaNs (missing info) with 0\n",
    "for col in asset_cols_to_use:\n",
    "    base_df[col] = pd.to_numeric(base_df[col], errors='coerce').fillna(0) # Assuming HCES data is 1/0, if not map 'Yes' to 1\n",
    "\n",
    "base_df['Asset_Score_X1'] = 0\n",
    "for col in asset_cols_to_use:\n",
    "    weight = asset_weights.get(col, 0)\n",
    "    base_df['Asset_Score_X1'] += base_df[col] * weight\n",
    "\n",
    "print(f\"Engineered Asset_Score_X1 using {len(asset_cols_to_use)} columns.\")\n",
    "\n",
    "\n",
    "# 2. RATION & SCHEME INDEX (X2): Simple Sum of Social Benefits\n",
    "\n",
    "scheme_cols = [\n",
    "    'Benefitted_From_PMGKY',\n",
    "    'Ayushman_beneficiary',\n",
    "    'LPG_subsidized_cylinders',\n",
    "    'LPG_subsidy_received',\n",
    "    'Medical_benefit_received',\n",
    "    'Ration_Any_Item_Last_30_Days',\n",
    "    # Add other binary scheme/ration columns from your data here\n",
    "]\n",
    "scheme_cols_to_use = base_df.columns.intersection(scheme_cols).tolist()\n",
    "\n",
    "# Ensure scheme columns are numeric (1/0) and fill NaNs (missing info) with 0\n",
    "for col in scheme_cols_to_use:\n",
    "    base_df[col] = pd.to_numeric(base_df[col], errors='coerce').fillna(0) # Assuming HCES data is 1/0\n",
    "\n",
    "base_df['Scheme_Index_X2'] = base_df[scheme_cols_to_use].sum(axis=1)\n",
    "\n",
    "print(f\"Engineered Scheme_Index_X2 using {len(scheme_cols_to_use)} columns.\")\n",
    "\n",
    "# =========================================================================\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- STEP 4 COMPLETE: EXPENDITURE FEATURES ADDED ---\")\n",
    "\n",
    "\n",
    "# --- STEP 5: CALCULATE TARGET VARIABLE, CLEAN, AND SAVE ---\n",
    "\n",
    "# a. Calculate target variable\n",
    "size_col = 'household_size_calculated' if 'household_size_calculated' in base_df.columns else 'HOUSEHOLD_SIZE'\n",
    "base_df['MPCE'] = base_df['MONTHLY_CONSUMPTION_EXP'] / base_df[size_col]\n",
    "base_df['MPCE'] = base_df['MPCE'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# b. <<< THE CRUCIAL FIX >>>\n",
    "# Drop all rows where our key target variable or its source is missing.\n",
    "# This ensures the model is only trained on valid data.\n",
    "print(f\"\\nShape before dropping missing target values: {base_df.shape}\")\n",
    "base_df.dropna(subset=['MONTHLY_CONSUMPTION_EXP', 'MPCE'], inplace=True)\n",
    "print(f\"Shape after dropping missing target values: {base_df.shape}\")\n",
    "\n",
    "# c. Save the cleaned file\n",
    "base_df.to_parquet('master_dataset.parquet', index=False)\n",
    "\n",
    "print(\"\\n--- STEP 5 COMPLETE: TARGET VARIABLE CREATED & CLEANED FILE SAVED ---\")\n",
    "print(\"\\n--- DATA ASSEMBLY IS NOW 100% COMPLETE! ---\")\n",
    "print(\"\\nFinal assembled data sample:\")\n",
    "print(base_df[['Scheme_Index_X2','Asset_Score_X1','household_size_calculated', 'head_of_household_age', 'fuel_expenditure', 'comm_expenditure', 'MPCE']].head())\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics of Final Assembled Data ---\")\n",
    "print(base_df[['Scheme_Index_X2','Asset_Score_X1','household_size_calculated', 'fuel_expenditure', 'comm_expenditure', 'MONTHLY_CONSUMPTION_EXP', 'MPCE']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b3ec35-ae37-4152-bbb5-d34e594d09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survey_Name', 'Year', 'FSU_Serial_No', 'Sector', 'State', 'NSS_Region', 'District', 'Stratum', 'Sub_stratum', 'Panel', 'Sub_sample', 'FOD_Sub_Region', 'Sample_SU_No', 'Sample_Sub_Division_No', 'Second_Stage_Stratum_No', 'Sample_Household_No', 'Questionnaire_No', 'Level', 'Survey_Code', 'Reason_for_Substitution_Code', 'Multiplier', 'Benefitted_From_PMGKY', 'Casual_Labour_Source_Sector', 'Dwelling_Unit_Exists', 'Energy_Source_Cooking', 'Energy_Source_Lighting', 'Engaged_in_Economic_Activity_Las', 'HH_Size_FDQ', 'Household_Type', 'Land_Ownership', 'Max_Income_Activity', 'NCO_2015_Code', 'NIC_2008_Code', 'Ration_Card_Type', 'Regular_Wage_Source_Sector', 'Religion_of_HH_Head', 'Rent_Rate_Available_Rural', 'Self_Employment_Source_Sector', 'Social_Group_of_HH_Head', 'Total_Area_Land_Owned_Acres', 'Type_of_Dwelling_Unit', 'Type_of_Land_Owned', 'Ceremony_Performed_Last_30_Days', 'Meals_Served_to_Non_HH_Members', 'Online_Dry_Fruits', 'Online_Egg_Fish_Meat', 'Online_Fresh_Fruits', 'Online_Groceries', 'Online_Milk', 'Online_Other_Food_Items', 'Online_Packed_Processed_Food', 'Online_Served_Processed_Food', 'Online_Vegetables', 'Ration_Any_Item_Last_30_Days', 'Ration_Coarse_Grain', 'Ration_Edible_Oil', 'Ration_Other_Food_Items', 'Ration_Pulses', 'Ration_Rice', 'Ration_Sugar', 'Ration_Wheat', 'Any_member_attended_school', 'Ayushman_beneficiary', 'Fee_waiver_received', 'Free_electricity', 'Free_other_items_received', 'Free_school_bag_received', 'Free_stationery_received', 'Free_textbooks_received', 'Hospitalization_case', 'Kerosene_ration_card', 'LPG_subsidized_cylinders', 'LPG_subsidy_received', 'Medical_benefit_amount', 'Medical_benefit_received', 'Num_ayushman_beneficiaries', 'Num_fee_waiver_received', 'Num_govt_school_attended', 'Num_medical_beneficiaries', 'Num_private_school_attended', 'Online_purchase_education', 'Online_purchase_fuel_light', 'Online_purchase_medicine', 'Online_purchase_services', 'Online_purchase_toilet_articles', 'Total_other_items', 'Total_school_bags', 'Total_stationery', 'Total_textbooks', 'Free_Bicycle', 'Free_Clothing', 'Free_Footwear', 'Free_Laptop', 'Free_Mobile', 'Free_Other', 'Free_Scooter', 'Free_Tablet', 'Num_Free_Bicycle', 'Num_Free_Clothing', 'Num_Free_Footwear', 'Num_Free_Laptop', 'Num_Free_Mobile', 'Num_Free_Other', 'Num_Free_Scooter', 'Num_Free_Tablet', 'Online_Bedding', 'Online_Clothing', 'Online_Crockery', 'Online_Footwear', 'Online_Furniture', 'Online_HouseholdAppliances', 'Online_MedicalEquipment', 'Online_Mobile', 'Online_PersonalGoods', 'Online_RecreationGoods', 'Online_SportsGoods', 'Possess_AirCooler', 'Possess_AnimalCart', 'Possess_Bicycle', 'Possess_Car', 'Possess_Laptop', 'Possess_Mobile', 'Possess_Radio', 'Possess_Refrigerator', 'Possess_Scooter', 'Possess_Television', 'Possess_Truck', 'Possess_WashingMachine', 'TV_Facility_Type', 'questionnaire_No', 'MONTHLY_CONSUMPTION_EXP', 'HOUSEHOLD_SIZE', 'household_size_calculated', 'head_of_household_age', 'avg_education_years_adults', 'num_internet_users', 'fuel_expenditure', 'comm_expenditure', 'Asset_Score_X1', 'Scheme_Index_X2', 'MPCE']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# You might need to install a library to read parquet files if you haven't already.\n",
    "# Open your Anaconda Prompt and run: pip install pyarrow\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "# 1. Load your cleaned .parquet file\n",
    "file_path = 'C:/Users/Yash Kumar/Desktop/SIH_Credit_Scoring/master_dataset.parquet' \n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# 2. Get the list of all column names\n",
    "column_list = df.columns.tolist()\n",
    "\n",
    "# 3. Print the full list\n",
    "print(column_list)\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75eed3e-ef6d-4d5e-94bc-47490146b62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
